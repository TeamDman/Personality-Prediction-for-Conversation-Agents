{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Personality Prediction via Large Language Models (LLMs)\n",
    "\n",
    "This notebook provides code to predict conversation personality traits using OpenAI’s GPT models. It runs in two stages:\n",
    "\n",
    "1. **Speaker Attribute Analysis**  \n",
    "2. **Personality Prediction**  \n",
    "\n",
    "> **Before you begin:**  \n",
    "> 1. Get an OpenAI API key and base URL.  \n",
    "> 2. Purchase the required quotas on [OpenAI’s API](https://openai.com/index/openai-api/).  \n",
    "> 3. Enter your credentials in the cell below (`api_key`, `api_base`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "############### API Key of ChatGPT ###############\n",
    "##################################################\n",
    "\n",
    "# api_key = \"sk_...\"\n",
    "# api_base = \"\"\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../sho_util/pyfiles/\")\n",
    "from basic import get_bool_base_on_conditions\n",
    "\n",
    "sys.path.append('../pyfiles/')\n",
    "from dialog import concatenate_close_voice, update_information, get_start_end_referencedf, most_frequent\n",
    "from llmprediction import GetResult_Personality, get_prompt_character\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 504)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "# Speaker Attribute Analysis\n",
    "\n",
    "In this step, we analyze each speaker’s textual, acoustic, and behavioral cues and map them to relative discrete values (e.g., “Normal”, “Rare”, “Frequent”). These values are normalized using the mean and IQR computed over the first 995 dialogue samples from the Fisher dataset (`mean_dir` and `iqr_dir`).  \n",
    "\n",
    "Please adjust the following variables as needed:\n",
    "\n",
    "- `audiopath`: A string containing the path to your two‑channel audio file.  \n",
    "- `feature_dir`: A string specifying the directory where all preprocessed outputs are saved.  \n",
    "- `mean_dir`: A string pointing to the folder with precomputed means used for normalization.  \n",
    "- `iqr_dir`: A string pointing to the folder with precomputed IQRs used for normalization.  \n",
    "\n",
    "You can also tweak these behavioral parameters:\n",
    "\n",
    "- `n_turns`: Number of turns per minute  \n",
    "- `avg_duration`: Average speaking time per turn  \n",
    "- `avg_laughter`: Laughter events per minute  \n",
    "- `avg_bc`: Total backchannel events per minute  \n",
    "- `avg_emotive`: Emotive backchannel events per minute  \n",
    "- `avg_cognitive`: Cognitive backchannel events per minute  \n",
    "- `n_ci`: Interjection events per minute  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################\n",
      "##### Preparation #####\n",
      "#######################\n",
      "####################################################\n",
      "##### Put Sentence-level Sentiment and Emotion #####\n",
      "####################################################\n",
      "##################################################\n",
      "##### Put Backchannel Classification Results #####\n",
      "##################################################\n",
      "##############################################\n",
      "### Put Sentence Labels on Each Text Token ###\n",
      "##############################################\n",
      "#####################################\n",
      "##### Obtain Speaker Attributes #####\n",
      "#####################################\n",
      "######################################\n",
      "##### Analyze Speaker Attributes #####\n",
      "######################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">basics</th>\n",
       "      <th colspan=\"7\" halign=\"left\">emotion</th>\n",
       "      <th colspan=\"3\" halign=\"left\">sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>speaker</th>\n",
       "      <th>n_turns</th>\n",
       "      <th>avg_duration</th>\n",
       "      <th>avg_laughter</th>\n",
       "      <th>avg_emotive</th>\n",
       "      <th>avg_cognitive</th>\n",
       "      <th>n_ci</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample</td>\n",
       "      <td>A</td>\n",
       "      <td>Many</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Very Frequent</td>\n",
       "      <td>Rare</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample</td>\n",
       "      <td>B</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Short</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Very Frequent</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    basics                                                           \\\n",
       "  filename speaker n_turns avg_duration avg_laughter    avg_emotive   \n",
       "0   sample       A    Many       Normal       Normal  Very Frequent   \n",
       "1   sample       B  Normal        Short       Normal         Normal   \n",
       "\n",
       "                         emotion                                        \\\n",
       "   avg_cognitive    n_ci   anger disgust fear  joy   neutral   sadness   \n",
       "0           Rare  Normal     0.0     0.1  0.0  0.1  0.400000  0.100000   \n",
       "1  Very Frequent  Normal     0.0     0.0  0.0  0.0  0.444444  0.444444   \n",
       "\n",
       "            sentiment                      \n",
       "   surprise  positive   neutral  negative  \n",
       "0  0.300000  0.100000  0.800000  0.100000  \n",
       "1  0.111111  0.222222  0.444444  0.333333  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "audiopath = \"../audio/sample.wav\"\n",
    "feature_dir = \"../audio/features/sample/\"\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "# These values represent the mean and iqr of the speaker attribute values computed from the first 995 samples in Fisher. \n",
    "mean_dir = {\n",
    "    'n_turns': 7.383293694948496,\n",
    "    'avg_duration': 3.3336954492304645,\n",
    "    'avg_laughter': 1.8922364427060465,\n",
    "    'avg_emotive': 0.8121129481276123,\n",
    "    'avg_cognitive': 1.9553241783407014,\n",
    "    'n_ci': 0.5467134601571795\n",
    "}\n",
    "iqr_dir = {\n",
    "    'n_turns': 2.605129382565053,\n",
    "    'avg_duration': 1.6044905337979838,\n",
    "    'avg_laughter': 2.4286077576460636,\n",
    "    'avg_emotive': 0.8618173313355536,\n",
    "    'avg_cognitive': 1.7463671894145367,\n",
    "    'n_ci': 0.5053406298424978\n",
    "}\n",
    "\n",
    "x, fs = librosa.load(audiopath)\n",
    "personality_list = [\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "\n",
    "score_dir = {\n",
    "    \"highly aligned\": 100,\n",
    "    \"aligned\": 50,\n",
    "    \"positive\": 100,\n",
    "    \"neutral\": 0,\n",
    "    \"opposed\": -50,\n",
    "    \"negative\": -100,\n",
    "    \"highly opposed\": -100,\n",
    "    \n",
    "    \"slightly opposed\": -50,\n",
    "    \"slightly aligned\": 50,\n",
    "    \"introversion\": -100,\n",
    "    \"emotionally stable\": -50,\n",
    "    \"antagonism\": -100,\n",
    "    \"aligned with introversion\": -50,\n",
    "    \n",
    "    \"Sentiment\": {\n",
    "        \"positive\": 10,\n",
    "        \"neutral\": 0,\n",
    "        \"negative\": -10,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"#######################\")\n",
    "print(\"##### Preparation #####\")\n",
    "print(\"#######################\")\n",
    "\n",
    "labels = {\n",
    "    \"Sentence-Emotion\": [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\",],\n",
    "    \"Sentence-Sentiment\": [\"positive\", \"neutral\", \"negative\"],\n",
    "}\n",
    "sentiment_score = False\n",
    "classes = [\"Sentence-Emotion\"]\n",
    "if not(sentiment_score):\n",
    "    classes += [\"Sentence-Sentiment\"]\n",
    "counts = {cl: {label: 0 for label in labels[cl]} for cl in classes}\n",
    "classes = []\n",
    "if sentiment_score:\n",
    "    classes += [\"Sentence-Sentiment\"]\n",
    "scores = {cl: 0 for cl in classes}\n",
    "columns = []\n",
    "for cl in [\"filename\", \"speaker\", \"n_turns\", \"duration\", \"n_newwords\", \"ratio_newwords\", \"avg_duration\", \"avg_laughter\", \"avg_emotive\", \"avg_cognitive\", \"avg_bc\", \"n_ci\"]:\n",
    "    columns += [(\"basics\", cl)]\n",
    "for key in counts:\n",
    "    cl1 = key.split(\"-\")[-1].lower()\n",
    "    for cl2 in counts[key]:\n",
    "        columns += [(cl1, cl2)]\n",
    "for key in scores:\n",
    "    cl1 = key.split(\"-\")[-1].lower()\n",
    "    columns += [(\"scores\", cl1)]\n",
    "columns = pd.MultiIndex.from_tuples(columns)\n",
    "\n",
    "thres_duration_tobe_bc = 1.0\n",
    "thres_being_bc = 1.5\n",
    "\n",
    "resultpath = feature_dir + \"whisper/\" + os.path.basename(audiopath[:-4]) + f\".npy\"\n",
    "laughpath = feature_dir + \"laughs/\" + os.path.basename(audiopath[:-4]) + f\".npy\"\n",
    "tablepath = laughpath.replace(\"laughs\", \"results\")\n",
    "\n",
    "a = np.load(tablepath, allow_pickle=True).item()\n",
    "rawdata, data1, data2, data3, dfci = a[\"rawdata\"], a[\"data1\"], a[\"data2\"], a[\"data3\"], a[\"dfci\"]\n",
    "minutes = (rawdata[\"end\"].values.max()-rawdata[\"start\"].values.min())/60\n",
    "\n",
    "print(\"####################################################\")\n",
    "print(\"##### Put Sentence-level Sentiment and Emotion #####\")\n",
    "print(\"####################################################\")\n",
    "\n",
    "tt_classes = [\"Sentiment\", \"Emotion\"]\n",
    "dirname = feature_dir + \"classification_results/\"\n",
    "keys = [\"_\".join(os.path.basename(a[:-4]).split(\"_\")[1:3]) for a in glob.glob(dirname+\"classificationresults_*.npy\")]\n",
    "keys.sort()\n",
    "\n",
    "udfst = data1.copy()\n",
    "udfst[\"Sentence-Emotion\"] = \"\"\n",
    "udfst[\"Sentence-Sentiment\"] = np.nan\n",
    "for key in keys:\n",
    "    # Obtain the prediction results\n",
    "    path = dirname + f\"classificationresults_{key}.npy\"\n",
    "    hey = np.load(path, allow_pickle=True).item()\n",
    "    result_te, result_ts = hey[\"result_te\"], hey[\"result_ts\"]\n",
    "    a = [a[\"label\"] for a in result_te[0]]\n",
    "    b = [a[\"score\"] for a in result_te[0]]\n",
    "    result_te = a[np.argmax(b)]\n",
    "    result_ts = result_ts[0][\"label\"]\n",
    "\n",
    "    # Insert the information to udfst\n",
    "    startllm, endllm = [int(a) for a in key.split(\"_\")]\n",
    "    try:\n",
    "        start = np.arange(len(udfst))[np.abs(rawdata.iloc[startllm][\"start\"]-udfst[\"start\"])<1e-5]\n",
    "        end = np.arange(len(udfst))[np.abs(rawdata.iloc[endllm][\"end\"]-udfst[\"end\"])<1e-5]\n",
    "        idx = list(set(list([udfst.iloc[a].name for a in start])) & set(list([udfst.iloc[a].name for a in end])))[0]\n",
    "        udfst.loc[idx, \"Sentence-Emotion\"] = result_te\n",
    "        udfst.loc[idx, \"Sentence-Sentiment\"] = score_dir[\"Sentiment\"][result_ts]\n",
    "        if not(sentiment_score):\n",
    "            udfst.loc[udfst[\"Sentence-Sentiment\"]==10, \"Sentence-Sentiment\"] = \"positive\"\n",
    "            udfst.loc[udfst[\"Sentence-Sentiment\"]==0, \"Sentence-Sentiment\"] = \"neutral\"\n",
    "            udfst.loc[udfst[\"Sentence-Sentiment\"]==-10, \"Sentence-Sentiment\"] = \"negative\"\n",
    "    except IndexError:\n",
    "        continue\n",
    "        \n",
    "print(\"##################################################\")\n",
    "print(\"##### Put Backchannel Classification Results #####\")\n",
    "print(\"##################################################\")\n",
    "\n",
    "dfbc = data2[data2[\"BC-Candidates\"]]\n",
    "# model = \"gpt-4o\"\n",
    "model = \"gemma3:12b\"\n",
    "\n",
    "addname = \"\" if model==\"\" else \"_\"+model.split(\"-\")[-1].replace(\":\", \"_\")\n",
    "\n",
    "dirname = feature_dir + f'LLM_responses{addname}/' + os.path.basename(audiopath)[:-4] + \"/\"\n",
    "tt_classes = [\"interjection type\"]\n",
    "keys = [\"_\".join(os.path.basename(a).split(\"_\")[1:3]) for a in glob.glob(dirname+\"backchannel_*_0.npy\")]\n",
    "keys.sort()\n",
    "\n",
    "udfbc = dfbc.copy()\n",
    "udfbc[\"BC-Labels\"] = \"\"\n",
    "for cl in tt_classes[1:]:\n",
    "    udfbc[\"BC-\"+cl] = \"\"\n",
    "for key in keys:\n",
    "    # Obtain the prediction results\n",
    "    paths = glob.glob(dirname+f\"backchannel_{key}_*.npy\")\n",
    "    results = {cl.lower(): [] for cl in tt_classes}\n",
    "    for path in paths:\n",
    "        a = np.load(path, allow_pickle=True).item()\n",
    "\n",
    "        for cl in tt_classes:\n",
    "            exist = cl in a\n",
    "            if not(exist):\n",
    "                cl = cl.lower()\n",
    "                exist = cl in a\n",
    "            if exist:\n",
    "                try:\n",
    "                    results[cl] += [a[cl].lower()]\n",
    "                except AttributeError:\n",
    "                    results[cl] += [key.lower() for key in a[cl]]\n",
    "\n",
    "    if len(results[tt_classes[0]])==0:\n",
    "        continue\n",
    "\n",
    "    summary = {}\n",
    "    for freq_key in [a.lower() for a in tt_classes[:2]]:\n",
    "        summary[freq_key] = most_frequent(results[freq_key])\n",
    "    for score_key in [a.lower() for a in tt_classes[2:]]:\n",
    "        summary[score_key] = np.mean([score_dir[score_key[0].upper()+score_key[1:]][a] for a in results[score_key]])\n",
    "\n",
    "    # Insert the information to udfbc\n",
    "    startllm, endllm = [int(a) for a in key.split(\"_\")]\n",
    "    try:\n",
    "        start = np.arange(len(udfbc))[np.abs(rawdata.iloc[startllm][\"start\"]-udfbc[\"start\"])<1e-5][0]\n",
    "        idx = udfbc.iloc[start].name\n",
    "        udfbc.loc[idx, \"BC-Labels\"] = summary[\"interjection type\"]\n",
    "        for cl in tt_classes[1:]:\n",
    "            udfbc.loc[idx, \"BC-\"+cl] = summary[cl.lower()]\n",
    "    except IndexError:\n",
    "        continue\n",
    "        \n",
    "print(\"##############################################\")\n",
    "print(\"### Put Sentence Labels on Each Text Token ###\")\n",
    "print(\"##############################################\")\n",
    "\n",
    "sldata = rawdata.copy()\n",
    "sldata[\"Sentence Label\"] = \"\"\n",
    "sldata_dir = {\n",
    "    \"A\": sldata[get_bool_base_on_conditions(sldata, {\"speaker\": [\"A\"]})],\n",
    "    \"B\": sldata[get_bool_base_on_conditions(sldata, {\"speaker\": [\"B\"]})],\n",
    "}\n",
    "\n",
    "slidx = list(sldata.columns).index(\"Sentence Label\")\n",
    "# Interjections (backchanneling)\n",
    "df = udfbc[get_bool_base_on_conditions(udfbc, {\"BC-Labels\": [\"cognitive\", \"emotive\", \"not backchannel\"]})]\n",
    "for i in range(len(df)):\n",
    "    array = df.iloc[i]\n",
    "    speaker = array[\"speaker\"]\n",
    "    start, end = get_start_end_referencedf(sldata_dir[speaker], array)\n",
    "    sldata_dir[speaker].iloc[start:end+1, slidx] = np.array([\"interjection-\"+array[\"BC-Labels\"]+\"-\"+str(i) for i in range(end-start+1)])\n",
    "\n",
    "if type(dfci)!=type(None):\n",
    "    # Interjections (controlling)\n",
    "    df = dfci.copy()\n",
    "    for i in range(len(df)):\n",
    "        array = df.iloc[i]\n",
    "        speaker = array[\"speaker\"]\n",
    "        start, end = get_start_end_referencedf(sldata_dir[speaker], array)\n",
    "        sldata_dir[speaker].iloc[start:end+1, slidx] = np.array([\"interjection-controlling-\"+str(i) for i in range(end-start+1)])\n",
    "\n",
    "# Turn-taking\n",
    "df = udfst.copy()\n",
    "for i in range(len(df)):\n",
    "    array = df.iloc[i]\n",
    "    speaker = array[\"speaker\"]\n",
    "    start, end = get_start_end_referencedf(sldata_dir[speaker], array)\n",
    "    sl_list = sldata_dir[speaker].iloc[start:end+1, slidx].values\n",
    "    start_list = [0] if sldata_dir[speaker].iloc[start, slidx]==\"\" else []\n",
    "    start_list += [idx+2 for idx in range(len(sl_list[1:])) if sl_list[idx+1]!=\"\"]\n",
    "    for j in range(len(start_list)):\n",
    "        sentence_start = start_list[j]\n",
    "        if sentence_start>=len(sl_list):\n",
    "            break\n",
    "        try:\n",
    "            sentence_end = start_list[j+1]-1\n",
    "        except IndexError:\n",
    "            sentence_end = len(sl_list)\n",
    "        # sldata_dir[speaker].iloc[start+sentence_start:start+sentence_end, slidx] = np.array([\"turntaking-\"+array[\"Sentence-Labels\"]+\"-\"+str(i) for i in range(sentence_end-sentence_start)])\n",
    "        sldata_dir[speaker].iloc[start+sentence_start:start+sentence_end, slidx] = np.array([\"turntaking-\"+str(i) for i in range(sentence_end-sentence_start)])\n",
    "\n",
    "sldata = pd.concat([sldata_dir[\"A\"], sldata_dir[\"B\"]], axis=0).loc[np.arange(len(sldata))]\n",
    "        \n",
    "print(\"#####################################\")\n",
    "print(\"##### Obtain Speaker Attributes #####\")\n",
    "print(\"#####################################\")\n",
    "\n",
    "arrays = []\n",
    "df = udfst.copy()\n",
    "nturns = {speaker: get_bool_base_on_conditions(df, {\"speaker\": [speaker]}).sum() for speaker in [\"A\", \"B\"]}\n",
    "for speaker in [\"A\", \"B\"]:\n",
    "    dfspk = data2[get_bool_base_on_conditions(data2, {\"speaker\":[speaker]})]\n",
    "    rawspk = sldata[get_bool_base_on_conditions(sldata, {\"speaker\": [speaker]})]\n",
    "    dflaugh = sldata[get_bool_base_on_conditions(sldata, {\"speaker\": [speaker], \"transcription\":[\"[Laugh]\", \"[StartLaugh]\"]})]\n",
    "    stspk = udfst[get_bool_base_on_conditions(udfst, {\"speaker\": [speaker]})]\n",
    "    bcspk = udfbc[get_bool_base_on_conditions(udfbc, {\"speaker\": [speaker]})]\n",
    "    if type(dfci)!=type(None):\n",
    "        cispk = dfci[get_bool_base_on_conditions(dfci, {\"speaker\": [speaker]})]\n",
    "    else:\n",
    "        cispk = []\n",
    "\n",
    "    # vocab diversity\n",
    "    vocabfull = list(rawspk.transcription)\n",
    "    vocab = len(set(vocabfull))\n",
    "    dur = dfspk.duration.values.sum()/60\n",
    "    n = vocab/dur\n",
    "\n",
    "    # average duration per response\n",
    "    df1spk = data1[get_bool_base_on_conditions(data1, {\"speaker\":[speaker]})]\n",
    "    dur1 = df1spk.duration.values.sum()\n",
    "    avgdur = dur1/nturns[speaker]\n",
    "\n",
    "    # average Laugh\n",
    "    avglaugh = len(dflaugh)/dfspk.duration.values.sum()*60\n",
    "\n",
    "    temdf = stspk[stspk[\"duration\"]>=thres_being_bc].copy()\n",
    "    classes = [\"Sentence-Emotion\"] # Both turn-taking and interjections\n",
    "    if not(sentiment_score):\n",
    "        classes += [\"Sentence-Sentiment\"]\n",
    "    counts = {cl: {label: 0 for label in labels[cl]} for cl in classes}\n",
    "    for cl in classes:\n",
    "        for dfref in [temdf]:\n",
    "            df = dfref.loc[:, [cl, \"speaker\"]].groupby([cl]).count()\n",
    "            for key in df.index:\n",
    "                if not(key in labels[cl]):\n",
    "                    continue\n",
    "                counts[cl][key] += df.loc[key][0]\n",
    "\n",
    "    for cl in counts:\n",
    "        total = np.array(list(counts[cl].values())).sum()\n",
    "        for key in counts[cl]:\n",
    "            counts[cl][key] = counts[cl][key]/total\n",
    "\n",
    "    ### Get the scores for other text features\n",
    "    if not(sentiment_score):\n",
    "        df = temdf.loc[:, [\"Sentence-Sentiment\"]]\n",
    "        df[df==\"\"] = np.nan\n",
    "        if pd.isna(df).values.mean()==0:\n",
    "            scores = dict(df.mean())\n",
    "\n",
    "    stdiffspk = udfst[get_bool_base_on_conditions(udfst, {\"speaker\": list(set([\"A\", \"B\"])-set([speaker]))})]\n",
    "    stdiffspk = stdiffspk[stdiffspk[\"duration\"]>=thres_duration_tobe_bc]\n",
    "    total = stdiffspk[\"duration\"].sum()/60\n",
    "    avgemobc = (get_bool_base_on_conditions(bcspk, {\"BC-Labels\": [\"emotive\"]}).sum())/total\n",
    "    avgcogbc = (get_bool_base_on_conditions(bcspk, {\"BC-Labels\": [\"cognitive\"]}).sum())/total\n",
    "    avgallbc = avgemobc + avgcogbc\n",
    "    ratiobc = [avgemobc/avgallbc, avgcogbc/avgallbc]\n",
    "    nci = len(cispk) + (udfbc[\"BC-Labels\"]==\"not backchannel\").sum() # number of controlling interjections (successful and unsuccessful)\n",
    "\n",
    "    array = [\n",
    "        os.path.basename(audiopath)[:-4], speaker, nturns[speaker]/minutes, dur, vocab, vocab/len(vocabfull), avgdur, avglaugh, avgemobc, avgcogbc, avgallbc, nci/minutes,\n",
    "    ]\n",
    "    for key in counts:\n",
    "        for cl in counts[key]:\n",
    "            array += [counts[key][cl]]\n",
    "    for key in scores:\n",
    "        array += [scores[key]]\n",
    "    arrays += [array]\n",
    "\n",
    "data = pd.DataFrame(np.array(arrays), columns=columns)\n",
    "data.loc[:, columns[2:]] = data.loc[:, columns[2:]].values.astype(float)\n",
    "data = data.loc[:, [(\"basics\", cl) for cl in [\"filename\", \"speaker\"]+list(mean_dir.keys())]+list(data[[\"emotion\"]].columns)+list(data[[\"sentiment\"]].columns)]\n",
    "\n",
    "print(\"######################################\")\n",
    "print(\"##### Analyze Speaker Attributes #####\")\n",
    "print(\"######################################\")\n",
    "\n",
    "dvcolumns = []\n",
    "for cl in mean_dir:\n",
    "    dvcolumns += [(\"basics\", cl)]\n",
    "dvcolumns = pd.MultiIndex.from_tuples(dvcolumns)\n",
    "\n",
    "cl2name = {\n",
    "    \"n_turns\": \"Number of turns\", # Per minute\n",
    "    \"duration\": \"Total talking time\",\n",
    "    \"n_newwords\": \"Number of new words used in the conversation\",\n",
    "    \"ratio_newwords\": \"Frequency of new words usage\",\n",
    "    \"avg_duration\": \"Talking time per turn\",\n",
    "    \"avg_laughter\": \"Frequency of Laughter\",\n",
    "    \"avg_bc\": \"Frequency of Backchannels\",\n",
    "    \"avg_emotive\": \"Frequency of Emotive Backchannel\",\n",
    "    \"avg_cognitive\": \"Frequency of Cognitive Backchannel\",\n",
    "    \"n_ci\": \"Frequency of interjections\" # Per minute\n",
    "}\n",
    "eval2step = {\n",
    "    \"samples\": \"Summarize the sample responses.\",\n",
    "    \"basics\": \"Summarize the basic statistics.\",\n",
    "    \"emotion\": \"Summarize the emotion distribution.\",\n",
    "    \"sentiment\": \"Summarize the sentiment scores\",\n",
    "}\n",
    "\n",
    "# add = \"scores\" if sentiment_score else \"sentiment\"\n",
    "# averages = {cl: data[cl].mean() for cl in [\"emotion\", add]}\n",
    "\n",
    "ratio1 = 0.8\n",
    "ratio2 = 1.5*ratio1\n",
    "\n",
    "status = data.copy()\n",
    "status.loc[:, dvcolumns] = \"Normal\"\n",
    "for cl in dvcolumns:\n",
    "    if cl[1] in [\"duration\", \"avg_duration\"]:\n",
    "        high_text, low_text = \"Long\", \"Short\"\n",
    "        high_emp, low_emp = \"Very\", \"Very\"\n",
    "    elif cl[1] in [\"n_turns\", \"n_newwords\"]:\n",
    "        high_text, low_text = \"Many\", \"Few\"\n",
    "        high_emp, low_emp = \"So\", \"Very\"\n",
    "    elif cl[1] in [\"ratio_newwords\", \"avg_laughter\", \"avg_emotive\", \"avg_cognitive\", \"avg_bc\", \"n_ci\"]:\n",
    "        high_text, low_text = \"Frequent\", \"Rare\"\n",
    "        high_emp, low_emp = \"Very\", \"Very\"\n",
    "    else:\n",
    "        assert False\n",
    "    values = data[cl].values\n",
    "    mean = mean_dir[cl[1]]\n",
    "    iqr = iqr_dir[cl[1]]\n",
    "    # print(cl, mean, iqr)\n",
    "    status.loc[values>mean+ratio1*iqr, cl] = high_text\n",
    "    status.loc[values>mean+ratio2*iqr, cl] = \"Very \" + high_text\n",
    "    status.loc[values<mean-ratio1*iqr, cl] = low_text\n",
    "    status.loc[values<mean-ratio2*iqr, cl] = \"Very \" + low_text\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "# Personality Prediction\n",
    "\n",
    "In this step, we feed the analyzed speaker attributes into GPT models to predict each speaker’s personality profile. The prompt includes:\n",
    "\n",
    "- `samples`: Example utterances from the speaker  \n",
    "- `emotion`: Distribution of emotion labels  \n",
    "- `sentiment`: Distribution of sentiment scores  \n",
    "- `basics`: Behavioral attributes from the previous step  \n",
    "\n",
    "Please adjust the following variables as needed:\n",
    "\n",
    "- `repeatnum`: An integer specifying how many times to query the LLM for each prediction (higher → more reliable).  \n",
    "- `model_list`: A list of GPT model names (e.g., `[\"gpt-4o\", \"gpt-4.1\"]`). To add more models, edit `gpt_api_no_stream` in `./sho_util/pyfiles/gpt.py`.  \n",
    "- `orders`: A list defining which attribute categories to include in the prompt (e.g., `[\"samples\", \"emotion\", \"sentiment\", \"basics\"]`).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "##### Personality Prediction #####\n",
      "##################################\n",
      "gpt-4o\n",
      "gpt-4.1\n",
      "gpt-4.1mini\n",
      "gpt-o4mini\n",
      "#####################################\n",
      "##### Display Prediction Result #####\n",
      "#####################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">condition</th>\n",
       "      <th colspan=\"5\" halign=\"left\">personality</th>\n",
       "      <th colspan=\"6\" halign=\"left\">basics</th>\n",
       "      <th colspan=\"7\" halign=\"left\">emotion</th>\n",
       "      <th colspan=\"3\" halign=\"left\">sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model name</th>\n",
       "      <th>samples</th>\n",
       "      <th>basics</th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>n_turns</th>\n",
       "      <th>avg_duration</th>\n",
       "      <th>avg_laughter</th>\n",
       "      <th>avg_emotive</th>\n",
       "      <th>avg_cognitive</th>\n",
       "      <th>n_ci</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">sample</th>\n",
       "      <th>A</th>\n",
       "      <td>4o</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>10.212766</td>\n",
       "      <td>3.440400</td>\n",
       "      <td>3.479471</td>\n",
       "      <td>2.552764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>4.1</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>10.212766</td>\n",
       "      <td>3.440400</td>\n",
       "      <td>3.479471</td>\n",
       "      <td>2.552764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>4.1mini</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>10.212766</td>\n",
       "      <td>3.440400</td>\n",
       "      <td>3.479471</td>\n",
       "      <td>2.552764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>o4mini</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>10.212766</td>\n",
       "      <td>3.440400</td>\n",
       "      <td>3.479471</td>\n",
       "      <td>2.552764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>4o</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.680851</td>\n",
       "      <td>1.585882</td>\n",
       "      <td>3.773815</td>\n",
       "      <td>0.920726</td>\n",
       "      <td>5.524353</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>4.1</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.680851</td>\n",
       "      <td>1.585882</td>\n",
       "      <td>3.773815</td>\n",
       "      <td>0.920726</td>\n",
       "      <td>5.524353</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>4.1mini</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>8.680851</td>\n",
       "      <td>1.585882</td>\n",
       "      <td>3.773815</td>\n",
       "      <td>0.920726</td>\n",
       "      <td>5.524353</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>o4mini</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>☑︎</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.680851</td>\n",
       "      <td>1.585882</td>\n",
       "      <td>3.773815</td>\n",
       "      <td>0.920726</td>\n",
       "      <td>5.524353</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          condition                                  personality  \\\n",
       "         model name samples basics emotion sentiment    openness   \n",
       "sample A         4o      ☑︎     ☑︎      ☑︎        ☑︎        20.0   \n",
       "       A        4.1      ☑︎     ☑︎      ☑︎        ☑︎        30.0   \n",
       "       A    4.1mini      ☑︎     ☑︎      ☑︎        ☑︎        30.0   \n",
       "       A     o4mini      ☑︎     ☑︎      ☑︎        ☑︎         0.0   \n",
       "       B         4o      ☑︎     ☑︎      ☑︎        ☑︎       -30.0   \n",
       "       B        4.1      ☑︎     ☑︎      ☑︎        ☑︎       -40.0   \n",
       "       B    4.1mini      ☑︎     ☑︎      ☑︎        ☑︎       -50.0   \n",
       "       B     o4mini      ☑︎     ☑︎      ☑︎        ☑︎       -40.0   \n",
       "\n",
       "                                                                      basics  \\\n",
       "         conscientiousness extraversion agreeableness neuroticism    n_turns   \n",
       "sample A             -10.0        100.0          90.0       -90.0  10.212766   \n",
       "       A             -10.0        100.0          70.0       -10.0  10.212766   \n",
       "       A              30.0         90.0          80.0       -50.0  10.212766   \n",
       "       A              20.0         90.0          70.0       -90.0  10.212766   \n",
       "       B             -20.0          0.0          40.0        60.0   8.680851   \n",
       "       B              20.0          0.0          50.0        50.0   8.680851   \n",
       "       B              10.0         30.0          40.0        40.0   8.680851   \n",
       "       B             -20.0        -40.0          50.0        50.0   8.680851   \n",
       "\n",
       "                                                                        \\\n",
       "         avg_duration avg_laughter avg_emotive avg_cognitive      n_ci   \n",
       "sample A     3.440400     3.479471    2.552764      0.000000  0.510638   \n",
       "       A     3.440400     3.479471    2.552764      0.000000  0.510638   \n",
       "       A     3.440400     3.479471    2.552764      0.000000  0.510638   \n",
       "       A     3.440400     3.479471    2.552764      0.000000  0.510638   \n",
       "       B     1.585882     3.773815    0.920726      5.524353  0.510638   \n",
       "       B     1.585882     3.773815    0.920726      5.524353  0.510638   \n",
       "       B     1.585882     3.773815    0.920726      5.524353  0.510638   \n",
       "       B     1.585882     3.773815    0.920726      5.524353  0.510638   \n",
       "\n",
       "         emotion                                                 sentiment  \\\n",
       "           anger disgust fear  joy   neutral   sadness  surprise  positive   \n",
       "sample A     0.0     0.1  0.0  0.1  0.400000  0.100000  0.300000  0.100000   \n",
       "       A     0.0     0.1  0.0  0.1  0.400000  0.100000  0.300000  0.100000   \n",
       "       A     0.0     0.1  0.0  0.1  0.400000  0.100000  0.300000  0.100000   \n",
       "       A     0.0     0.1  0.0  0.1  0.400000  0.100000  0.300000  0.100000   \n",
       "       B     0.0     0.0  0.0  0.0  0.444444  0.444444  0.111111  0.222222   \n",
       "       B     0.0     0.0  0.0  0.0  0.444444  0.444444  0.111111  0.222222   \n",
       "       B     0.0     0.0  0.0  0.0  0.444444  0.444444  0.111111  0.222222   \n",
       "       B     0.0     0.0  0.0  0.0  0.444444  0.444444  0.111111  0.222222   \n",
       "\n",
       "                              \n",
       "           neutral  negative  \n",
       "sample A  0.800000  0.100000  \n",
       "       A  0.800000  0.100000  \n",
       "       A  0.800000  0.100000  \n",
       "       A  0.800000  0.100000  \n",
       "       B  0.444444  0.333333  \n",
       "       B  0.444444  0.333333  \n",
       "       B  0.444444  0.333333  \n",
       "       B  0.444444  0.333333  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "repeatnum = 10\n",
    "model_list = [\"gpt-4o\", \"gpt-4.1\", \"gpt-4.1mini\", \"gpt-o4mini\"]\n",
    "orders = [\"samples\", \"emotion\", \"sentiment\", \"basics\"] # \"emotion\", \"basics\", \"labels\", \"samples\"\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "get_response = True\n",
    "\n",
    "print(\"##################################\")\n",
    "print(\"##### Personality Prediction #####\")\n",
    "print(\"##################################\")\n",
    "\n",
    "target_columns = [\"n_turns\", \"avg_duration\", \"avg_laughter\", \"avg_emotive\", \"avg_cognitive\", \"n_ci\"]\n",
    "if len(orders)>2:\n",
    "    nsamples = 20\n",
    "else:\n",
    "    nsamples = 30\n",
    "\n",
    "for model in model_list:\n",
    "    print(model)\n",
    "    addname = \"_\"+model.split(\"-\")[-1]\n",
    "    addname2 = \"\" if len(orders)>2 else \"_withoutemosentiment\"\n",
    "    for idx in range(2): # two speakers\n",
    "        array = status.iloc[idx]\n",
    "        spk = array[(\"basics\", \"speaker\")]\n",
    "\n",
    "        dirname = feature_dir + f'LLM_responses{addname}{addname2}/' + os.path.basename(audiopath)[:-4] + \"/\"\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "        a = glob.glob(dirname + f\"personalityprediction_{spk}_*.npy\")\n",
    "        a.sort()\n",
    "        b = {int(os.path.basename(path).split(\"_\")[-1][:-4]): path for path in a}\n",
    "        iter_list = list(set(list(range(repeatnum))) - set(list(b.keys())))\n",
    "        if len(iter_list)==0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        df = udfst.copy()\n",
    "        df = update_information(df) \n",
    "        df = concatenate_close_voice(df, np.inf) \n",
    "        df = update_information(df) \n",
    "        df = df[get_bool_base_on_conditions(df, {\"speaker\": [array[(\"basics\", \"speaker\")]]})]\n",
    "        # df = df[(10.0>=df.duration)*(df.duration>=5.0)]\n",
    "        df = df[(np.inf>=df.duration)*(df.duration>=2.0)]\n",
    "        np.random.seed(0)\n",
    "        df = df.iloc[np.random.choice(np.arange(len(df)), size=min(nsamples, len(df)), replace=False)]\n",
    "\n",
    "        prompt = get_prompt_character(array, target_columns, orders, cl2name, eval2step, df)\n",
    "        if get_response:\n",
    "            for r in iter_list:\n",
    "                savepath = dirname + f'personalityprediction_{spk}_{r}.npy'\n",
    "                if os.path.exists(savepath):\n",
    "                    continue\n",
    "                response = GetResult_Personality(client, prompt, model, display_print=True)\n",
    "                np.save(savepath, response)\n",
    "                \n",
    "print(\"#####################################\")\n",
    "print(\"##### Display Prediction Result #####\")\n",
    "print(\"#####################################\")\n",
    "\n",
    "allresults = {}\n",
    "non_defined = []\n",
    "should_be_deleted = []\n",
    "orders_list = [[\"samples\", \"emotion\", \"sentiment\", \"basics\"]]\n",
    "allinfo = [\"samples\", \"basics\", \"emotion\", \"sentiment\"]\n",
    "num = len(data)\n",
    "for orders in orders_list:\n",
    "    for model in model_list:\n",
    "        addname = \"_\"+model.split(\"-\")[-1]\n",
    "        if len(orders)==4:\n",
    "            addname2 = \"\"\n",
    "        elif len(orders)==2:\n",
    "            if \"emotion\" in orders:\n",
    "                addname2 = \"_onlyemosentiment\"\n",
    "            else:\n",
    "                addname2 = \"_withoutemosentiment\"\n",
    "        elif len(orders)==3:\n",
    "            if \"samples\" in orders:\n",
    "                addname2 = \"_withoutinterjections\"\n",
    "            else:\n",
    "                addname2 = \"_withoutsamples\"\n",
    "        elif len(orders)==1:\n",
    "            if \"samples\" in orders:\n",
    "                addname2 = \"_onlysamples\"\n",
    "            elif \"basics\" in orders:\n",
    "                addname2 = \"_basics\"\n",
    "        dirname = feature_dir + f'LLM_responses{addname}{addname2}/' + os.path.basename(audiopath)[:-4] + \"/\"\n",
    "        \n",
    "        meandata = data.copy()\n",
    "        for cl in personality_list:\n",
    "            meandata[(\"personality\", cl)] = \"\"\n",
    "        stddata = meandata.copy()\n",
    "        for idx in range(num):\n",
    "            array = data.iloc[idx]\n",
    "            fn = array[(\"basics\", \"filename\")]\n",
    "            spk = array[(\"basics\", \"speaker\")]\n",
    "            paths = glob.glob(dirname + f\"/personalityprediction_{spk}_*.npy\")\n",
    "            results = {cl: [] for cl in personality_list}\n",
    "            prediction_num = 0\n",
    "            for path in paths:\n",
    "                a = np.load(path, allow_pickle=True).item()\n",
    "                for cl in personality_list:\n",
    "                    exist = cl in a\n",
    "                    if not(exist):\n",
    "                        cl = cl.lower()\n",
    "                        exist = cl in a\n",
    "                    if exist:\n",
    "                        try:\n",
    "                            added_key = a[cl].lower()\n",
    "                            if added_key in score_dir:\n",
    "                                results[cl] += [added_key]\n",
    "                            else:\n",
    "                                non_defined += [added_key]\n",
    "                        except AttributeError:\n",
    "                            if type(a[cl])==dict:\n",
    "                                results[cl] += [key.lower() for key in a[cl].values()]\n",
    "                            else:\n",
    "                                try:\n",
    "                                    results[cl] += [key.lower() for key in a[cl]]\n",
    "                                except AttributeError:\n",
    "                                    results[cl] += [key.lower() for key in a[cl][0].values()]\n",
    "                prediction_num += 1\n",
    "            if prediction_num==0:\n",
    "                continue\n",
    "            for score_key in personality_list:\n",
    "                results[score_key] = results[score_key][:5]\n",
    "\n",
    "            mean = {}\n",
    "            std = {}\n",
    "            for score_key in personality_list:\n",
    "                results[score_key] = list(np.array(results[score_key])[np.array(results[score_key])!=\"\"])\n",
    "            for score_key in personality_list:\n",
    "                mean[score_key] = np.mean([score_dir[a] for a in results[score_key]])\n",
    "                std[score_key] = np.std([score_dir[a] for a in results[score_key]])\n",
    "\n",
    "            meandata.loc[idx, \"personality\"] = np.array(list(mean.values()))\n",
    "            stddata.loc[idx, \"personality\"] = np.array(list(std.values()))\n",
    "        meandata = meandata[[\"personality\", \"basics\", \"emotion\", \"sentiment\"]]\n",
    "        meandata = meandata[pd.isna(meandata[\"personality\"]).sum(axis=1)==0]\n",
    "        meandata = meandata[meandata[(\"personality\", \"openness\")]!=\"\"]\n",
    "        stddata = stddata[[\"personality\", \"basics\", \"emotion\", \"sentiment\"]]\n",
    "        stddata = stddata[pd.isna(stddata[\"personality\"]).sum(axis=1)==0]\n",
    "        stddata = stddata[stddata[(\"personality\", \"openness\")]!=\"\"]\n",
    "        meandata.loc[:, \"personality\"] = meandata.loc[:, \"personality\"].values.astype(float)\n",
    "        stddata.loc[:, \"personality\"] = stddata.loc[:, \"personality\"].values.astype(float)\n",
    "        key = \"-\".join([str(a in orders) for a in allinfo])\n",
    "        allresults[model+\"_\"+\"-\".join(orders)] = {}\n",
    "        allresults[model+\"_\"+\"-\".join(orders)][\"mean\"] = meandata\n",
    "        allresults[model+\"_\"+\"-\".join(orders)][\"std\"] = stddata\n",
    "        \n",
    "columns = []\n",
    "for cl in [\"model name\"]+allinfo:\n",
    "    columns += [(\"condition\", cl)]\n",
    "columns = pd.MultiIndex.from_tuples(columns)\n",
    "\n",
    "texts = []\n",
    "df_list = []\n",
    "for key in allresults:\n",
    "    df = allresults[key][\"mean\"].groupby([(\"basics\", \"filename\"), (\"basics\", \"speaker\")]).mean()\n",
    "    b, c = key.split(\"_\")\n",
    "    mn = b.split(\"-\")[1]\n",
    "    c = c.split(\"-\")\n",
    "    l = []\n",
    "    for hey in [hey in c for hey in allinfo]:\n",
    "        l += [\"☑︎\" if hey else \"\"]\n",
    "    texts += [[mn]+l]*2\n",
    "    df_list += [df]\n",
    "    \n",
    "dfs = pd.concat(df_list, axis=0)\n",
    "dfcon = pd.DataFrame(np.array(texts), columns=columns, index=pd.MultiIndex.from_tuples(list(df.index)*(len(texts)//2)))\n",
    "dfs.index = dfcon.index\n",
    "dfresult = pd.concat([dfcon, dfs], axis=1)\n",
    "dfresult.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
