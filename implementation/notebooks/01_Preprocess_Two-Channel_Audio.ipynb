{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing Two‑Channel Speech Data\n",
    "\n",
    "This notebook shows you how to preprocess a two‑channel speech dialogue file and produce:\n",
    "\n",
    "1. **Word‑level transcriptions** via [OpenAI Whisper](https://github.com/openai/whisper)\n",
    "2. **Laughter probability scores** for each word via our [Laughter Detector](https://github.com/jrgillick/laughter-detection)\n",
    "\n",
    "> **Before you begin:**  \n",
    "> Install both Whisper and the laughter detector by following the instructions in our [README](https://github.com/shinshoji01/Personality-Prediction-for-Conversation-Agents/tree/main/implementation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import whisper\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../sho_util/pyfiles/\")\n",
    "\n",
    "from sound import play_audio\n",
    "from basic import get_bool_base_on_conditions\n",
    "\n",
    "from tqdm import tqdm\n",
    "sys.path.append('./../../../laughter-detection/')\n",
    "sys.path.append('./../../../laughter-detection/utils/')\n",
    "import configs\n",
    "# import torch_utils\n",
    "\n",
    "sys.path.append('../pyfiles/')\n",
    "from dialog import GetLaughs, save_audio\n",
    "\n",
    "tempfile = \"temp.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "Edit the two variables below to point to your data and output folder:\n",
    "\n",
    "- `audiopath`: A string containing the our two‑channel audio file (e.g. WAV with separate speaker channels).\n",
    "- `feature_dir`: A string specifying the path of the directory where all preprocessed outputs will be saved.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "##########################################\n",
      "########## Whisper Transciption ##########\n",
      "##########################################\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "audiopath = \"../audio/sample.wav\"\n",
    "feature_dir = \"../audio/features/sample/\"\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "print(\"##########################################\")\n",
    "print(\"########## Whisper Transciption ##########\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "##### Model Preparation #####\n",
    "\n",
    "whisper_size = \"turbo\"\n",
    "whisper_dir = feature_dir + \"whisper/\"\n",
    "model = whisper.load_model(whisper_size, device=device)\n",
    "\n",
    "##### Get Transcriptions #####\n",
    "savepath = whisper_dir + os.path.basename(audiopath[:-4]) + f\".npy\"\n",
    "os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "a, fs = librosa.load(audiopath, sr=None, mono=False)\n",
    "save_audio(tempfile, a[0], fs)\n",
    "result1 = whisper.transcribe(model, tempfile, temperature=0.0, word_timestamps=True, condition_on_previous_text=False)\n",
    "save_audio(tempfile, a[1], fs)\n",
    "result2 = whisper.transcribe(model, tempfile, temperature=0.0, word_timestamps=True, condition_on_previous_text=False)\n",
    "np.save(savepath, [result1, result2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22300096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "########## Laughter Detection ##########\n",
      "########################################\n",
      "training with dropout=0.0\n",
      "Loading checkpoint at: ../../../laughter-detection/checkpoints/in_use/resnet_with_augmentation/best.pth.tar\n",
      "Loading checkpoint at step:  60600\n"
     ]
    }
   ],
   "source": [
    "print(\"########################################\")\n",
    "print(\"########## Laughter Detection ##########\")\n",
    "print(\"########################################\")\n",
    "\n",
    "##### Model Preparation #####\n",
    "laughter_dir = feature_dir + \"laughs/\"\n",
    "repo_dir = \"../../../laughter-detection/\"\n",
    "model_path = repo_dir + \"checkpoints/in_use/resnet_with_augmentation\"\n",
    "config = \"resnet_with_augmentation\"\n",
    "config = configs.CONFIG_MAP[config]\n",
    "sample_rate = 8000 # This is the defaul value used in the laughter detection.\n",
    "laugh_detector = GetLaughs(config, sample_rate, device, model_path)\n",
    "    \n",
    "##### Get Laughter #####\n",
    "savepath = laughter_dir + os.path.basename(audiopath[:-4]) + f\".npy\"\n",
    "os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "laughs = []\n",
    "a, _ = librosa.load(audiopath, sr=sample_rate, mono=False)\n",
    "for i in range(2):\n",
    "    save_audio(tempfile, a[i], sample_rate)\n",
    "    audio_path = tempfile\n",
    "    probs, fps = laugh_detector.get(audio_path)\n",
    "    output = np.concatenate([[fps], probs])\n",
    "    laughs += [output]\n",
    "np.save(savepath, laughs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laughs[0].shape: (5119,)\n",
      "laughs[1].shape: (5119,)\n"
     ]
    }
   ],
   "source": [
    "for idx, arr in enumerate(laughs):\n",
    "    print(f\"laughs[{idx}].shape: {arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99bf9a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00024414 -0.00024414  0.00048828  0.00048828  0.00048828 -0.00048828\n",
      "  -0.00048828  0.00048828  0.00048828 -0.00048828]\n",
      " [-0.00048828  0.00048828  0.00341797  0.00146484 -0.00048828  0.00097656\n",
      "   0.00170898  0.00170898  0.00097656  0.00170898]]\n"
     ]
    }
   ],
   "source": [
    "print(a[:, :10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
